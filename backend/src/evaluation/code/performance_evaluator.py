"""
AI Email Assistant ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ

ì´ë©”ì¼ ë¶„ì„, ë‹µë³€ ìƒì„±, ìš”ì•½ í’ˆì§ˆì„ ì¸¡ì •í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
"""

import json
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
import statistics

logger = logging.getLogger(__name__)


@dataclass
class AnalysisEvaluation:
    """ì´ë©”ì¼ ë¶„ì„ í‰ê°€ ê²°ê³¼"""
    email_id: int
    email_type_score: float  # 0-25
    importance_score_accuracy: float  # 0-25
    needs_reply_score: float  # 0-25
    sentiment_score: float  # 0-25
    total_score: float  # 0-100
    evaluation_notes: str = ""


@dataclass
class ReplyEvaluation:
    """ë‹µë³€ ìƒì„± í‰ê°€ ê²°ê³¼"""
    email_id: int
    context_understanding: float  # 0-25
    tone_consistency: float  # 0-25
    response_appropriateness: float  # 0-25
    korean_naturalness: float  # 0-25
    total_score: float  # 0-100
    evaluation_notes: str = ""


@dataclass
class SummaryEvaluation:
    """ìš”ì•½ í‰ê°€ ê²°ê³¼"""
    summary_date: str
    information_completeness: float  # 0-30
    conciseness: float  # 0-20
    accuracy: float  # 0-30
    readability: float  # 0-20
    total_score: float  # 0-100
    evaluation_notes: str = ""


class PerformanceEvaluator:
    """ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self):
        self.analysis_results: List[AnalysisEvaluation] = []
        self.reply_results: List[ReplyEvaluation] = []
        self.summary_results: List[SummaryEvaluation] = []

    # ========== ì´ë©”ì¼ ë¶„ì„ í‰ê°€ ==========

    def evaluate_analysis(
        self,
        email_id: int,
        ai_result: Dict,
        ground_truth: Dict
    ) -> AnalysisEvaluation:
        """
        ì´ë©”ì¼ ë¶„ì„ ê²°ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            email_id: ì´ë©”ì¼ ID
            ai_result: AI ë¶„ì„ ê²°ê³¼ {email_type, importance_score, needs_reply, sentiment}
            ground_truth: ì •ë‹µ ë°ì´í„° (ê°™ì€ í˜•ì‹)

        Returns:
            AnalysisEvaluation ê°ì²´
        """
        notes = []

        # 1. email_type í‰ê°€ (25ì )
        email_type_score = 25 if ai_result.get('email_type') == ground_truth.get('email_type') else 0
        if email_type_score == 0:
            notes.append(f"ìœ í˜• ë¶ˆì¼ì¹˜: AI={ai_result.get('email_type')}, ì •ë‹µ={ground_truth.get('email_type')}")

        # 2. importance_score í‰ê°€ (25ì ) - Â±2 ì´ë‚´ë©´ ë§Œì , Â±3ì´ë©´ 15ì , ê·¸ ì™¸ 0ì 
        ai_importance = int(ai_result.get('importance_score', 0) or 0)
        gt_importance = int(ground_truth.get('importance_score', 0) or 0)
        importance_diff = abs(ai_importance - gt_importance)

        if importance_diff <= 2:
            importance_score_accuracy = 25
        elif importance_diff <= 3:
            importance_score_accuracy = 15
        else:
            importance_score_accuracy = 0
            notes.append(f"ì¤‘ìš”ë„ ì°¨ì´ í¼: AI={ai_importance}, ì •ë‹µ={gt_importance}")

        # 3. needs_reply í‰ê°€ (25ì )
        ai_needs_reply = str(ai_result.get('needs_reply', '')).lower() in ['true', '1', 'yes']
        gt_needs_reply = str(ground_truth.get('needs_reply', '')).lower() in ['true', '1', 'yes']
        needs_reply_score = 25 if ai_needs_reply == gt_needs_reply else 0
        if needs_reply_score == 0:
            notes.append(f"ë‹µë³€í•„ìš” ë¶ˆì¼ì¹˜: AI={ai_needs_reply}, ì •ë‹µ={gt_needs_reply}")

        # 4. sentiment í‰ê°€ (25ì )
        sentiment_score = 25 if ai_result.get('sentiment') == ground_truth.get('sentiment') else 0
        if sentiment_score == 0:
            notes.append(f"ê°ì • ë¶ˆì¼ì¹˜: AI={ai_result.get('sentiment')}, ì •ë‹µ={ground_truth.get('sentiment')}")

        total_score = email_type_score + importance_score_accuracy + needs_reply_score + sentiment_score

        evaluation = AnalysisEvaluation(
            email_id=email_id,
            email_type_score=email_type_score,
            importance_score_accuracy=importance_score_accuracy,
            needs_reply_score=needs_reply_score,
            sentiment_score=sentiment_score,
            total_score=total_score,
            evaluation_notes="; ".join(notes)
        )

        self.analysis_results.append(evaluation)
        return evaluation

    # ========== ë‹µë³€ ìƒì„± í‰ê°€ (LLM-as-Judge) ==========

    def create_reply_evaluation_prompt(
        self,
        original_email: Dict,
        generated_reply: str,
        target_tone: str
    ) -> str:
        """ë‹µë³€ í‰ê°€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        return f"""ë‹¤ìŒ ì´ë©”ì¼ì— ëŒ€í•œ AI ìƒì„± ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

## ì›ë³¸ ì´ë©”ì¼
- ë°œì‹ ì: {original_email.get('sender_name', 'Unknown')} <{original_email.get('sender_address', '')}>
- ì œëª©: {original_email.get('subject', '')}
- ë³¸ë¬¸:
{original_email.get('body_text', '')[:1000]}

## AI ìƒì„± ë‹µë³€ (ëª©í‘œ í†¤: {target_tone})
{generated_reply}

## í‰ê°€ ê¸°ì¤€ (ê° í•­ëª© 0-25ì )

1. **ë¬¸ë§¥ ì´í•´ë„** (0-25ì ): ì›ë³¸ ì´ë©”ì¼ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í–ˆëŠ”ê°€?
2. **í†¤ ì¼ê´€ì„±** (0-25ì ): ìš”ì²­ëœ í†¤({target_tone})ì„ ì˜ ìœ ì§€í–ˆëŠ”ê°€?
3. **ì‘ë‹µ ì ì ˆì„±** (0-25ì ): ì´ë©”ì¼ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œ ì ì ˆí•œê°€? (ì§ˆë¬¸ì— ë‹µë³€, ìš”ì²­ ì²˜ë¦¬ ë“±)
4. **í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ì›€** (0-25ì ): ë¬¸ë²•, ì–´íœ˜, ê²½ì–´ì²´ ì‚¬ìš©ì´ ìì—°ìŠ¤ëŸ¬ìš´ê°€?

## ì¶œë ¥ í˜•ì‹ (JSON)
```json
{{
    "context_understanding": <ì ìˆ˜>,
    "tone_consistency": <ì ìˆ˜>,
    "response_appropriateness": <ì ìˆ˜>,
    "korean_naturalness": <ì ìˆ˜>,
    "total_score": <ì´ì >,
    "evaluation_notes": "<í‰ê°€ ì˜ê²¬>"
}}
```
"""

    def parse_reply_evaluation(
        self,
        email_id: int,
        llm_response: str
    ) -> ReplyEvaluation:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ReplyEvaluation ìƒì„±"""

        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', llm_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì°¾ê¸°
                json_str = llm_response

            data = json.loads(json_str)

            evaluation = ReplyEvaluation(
                email_id=email_id,
                context_understanding=float(data.get('context_understanding', 0)),
                tone_consistency=float(data.get('tone_consistency', 0)),
                response_appropriateness=float(data.get('response_appropriateness', 0)),
                korean_naturalness=float(data.get('korean_naturalness', 0)),
                total_score=float(data.get('total_score', 0)),
                evaluation_notes=data.get('evaluation_notes', '')
            )

            self.reply_results.append(evaluation)
            return evaluation

        except Exception as e:
            logger.error(f"ë‹µë³€ í‰ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ReplyEvaluation(
                email_id=email_id,
                context_understanding=0,
                tone_consistency=0,
                response_appropriateness=0,
                korean_naturalness=0,
                total_score=0,
                evaluation_notes=f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
            )

    # ========== í†µê³„ ë° ì‹œê°í™” ==========

    def get_analysis_statistics(self) -> Dict:
        """ì´ë©”ì¼ ë¶„ì„ í‰ê°€ í†µê³„"""
        if not self.analysis_results:
            return {"message": "í‰ê°€ ë°ì´í„° ì—†ìŒ"}

        total_scores = [r.total_score for r in self.analysis_results]

        return {
            "count": len(self.analysis_results),
            "average_score": round(statistics.mean(total_scores), 2),
            "median_score": round(statistics.median(total_scores), 2),
            "min_score": min(total_scores),
            "max_score": max(total_scores),
            "std_dev": round(statistics.stdev(total_scores), 2) if len(total_scores) > 1 else 0,
            "breakdown": {
                "email_type_avg": round(statistics.mean([r.email_type_score for r in self.analysis_results]), 2),
                "importance_avg": round(statistics.mean([r.importance_score_accuracy for r in self.analysis_results]), 2),
                "needs_reply_avg": round(statistics.mean([r.needs_reply_score for r in self.analysis_results]), 2),
                "sentiment_avg": round(statistics.mean([r.sentiment_score for r in self.analysis_results]), 2)
            }
        }

    def get_reply_statistics(self) -> Dict:
        """ë‹µë³€ ìƒì„± í‰ê°€ í†µê³„"""
        if not self.reply_results:
            return {"message": "í‰ê°€ ë°ì´í„° ì—†ìŒ"}

        total_scores = [r.total_score for r in self.reply_results]

        return {
            "count": len(self.reply_results),
            "average_score": round(statistics.mean(total_scores), 2),
            "median_score": round(statistics.median(total_scores), 2),
            "min_score": min(total_scores),
            "max_score": max(total_scores),
            "breakdown": {
                "context_understanding_avg": round(statistics.mean([r.context_understanding for r in self.reply_results]), 2),
                "tone_consistency_avg": round(statistics.mean([r.tone_consistency for r in self.reply_results]), 2),
                "response_appropriateness_avg": round(statistics.mean([r.response_appropriateness for r in self.reply_results]), 2),
                "korean_naturalness_avg": round(statistics.mean([r.korean_naturalness for r in self.reply_results]), 2)
            }
        }

    def generate_comparison_report(
        self,
        before_stats: Dict,
        after_stats: Dict,
        evaluation_type: str = "analysis"
    ) -> str:
        """ê°œì„  ì „í›„ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""

        before_avg = before_stats.get('average_score', 0)
        after_avg = after_stats.get('average_score', 0)
        improvement = after_avg - before_avg
        improvement_pct = (improvement / before_avg * 100) if before_avg > 0 else 0

        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            {evaluation_type.upper()} ì„±ëŠ¥ ê°œì„  ë¹„êµ ë¦¬í¬íŠ¸              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘  ğŸ“Š ì „ì²´ ì ìˆ˜ ë¹„êµ                                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â•‘
â•‘  â”‚   í•­ëª©     â”‚   Before   â”‚   After    â”‚   ê°œì„ ìœ¨   â”‚       â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â•‘
â•‘  â”‚ í‰ê·  ì ìˆ˜  â”‚  {before_avg:>6.1f}ì   â”‚  {after_avg:>6.1f}ì   â”‚  {improvement_pct:>+6.1f}%  â”‚       â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â•‘
â•‘                                                              â•‘
"""

        if 'breakdown' in before_stats and 'breakdown' in after_stats:
            report += "â•‘  ğŸ“ˆ í•­ëª©ë³„ ìƒì„¸                                              â•‘\n"

            for key in before_stats['breakdown'].keys():
                before_val = before_stats['breakdown'].get(key, 0)
                after_val = after_stats['breakdown'].get(key, 0)
                item_improvement = after_val - before_val

                # ì‹œê°í™” ë°”
                bar_length = int(after_val)
                bar = "â–ˆ" * bar_length + "â–‘" * (25 - bar_length)

                report += f"â•‘  {key:<20} {bar} {after_val:>5.1f} ({item_improvement:>+5.1f}) â•‘\n"

        report += """â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        return report

    def export_results(self, filepath: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        data = {
            "exported_at": datetime.now().isoformat(),
            "analysis_evaluations": [asdict(r) for r in self.analysis_results],
            "reply_evaluations": [asdict(r) for r in self.reply_results],
            "summary_evaluations": [asdict(r) for r in self.summary_results],
            "statistics": {
                "analysis": self.get_analysis_statistics(),
                "reply": self.get_reply_statistics()
            }
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"í‰ê°€ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filepath}")


# ========== Ground Truth ìƒì„± ë„ìš°ë¯¸ ==========

def create_ground_truth_template(emails: List[Dict]) -> List[Dict]:
    """Ground Truth ì…ë ¥ì„ ìœ„í•œ í…œí”Œë¦¿ ìƒì„±"""

    templates = []
    for email in emails:
        templates.append({
            "email_id": email.get('id'),
            "subject": email.get('subject'),
            "sender": email.get('sender_address'),
            "body_preview": email.get('body_text', '')[:200],
            "ground_truth": {
                "email_type": "",  # ì±„ìš©/ë§ˆì¼€íŒ…/ê³µì§€/ê°œì¸/ê¸°íƒ€
                "importance_score": 0,  # 1-10
                "needs_reply": False,  # True/False
                "sentiment": ""  # positive/neutral/negative
            }
        })

    return templates


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
evaluator = PerformanceEvaluator()
