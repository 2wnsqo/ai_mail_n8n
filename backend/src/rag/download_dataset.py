"""
Enron Email Dataset ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

HuggingFaceì˜ lilac-enron-emails ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ 
RAGìš©ìœ¼ë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python -m src.rag.download_dataset
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import re

# HuggingFace datasets
try:
    from datasets import load_dataset
except ImportError:
    print("datasets íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install datasets")
    exit(1)


# ê²½ë¡œ ì„¤ì •
RAG_DIR = Path(__file__).parent
DATA_DIR = RAG_DIR / "data"


def clean_email_text(text: str) -> str:
    """ì´ë©”ì¼ í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""

    # ì´ë©”ì¼ í—¤ë” ì œê±° (From:, To:, Subject: ë“±)
    lines = text.split('\n')
    content_started = False
    content_lines = []

    for line in lines:
        # í—¤ë” ì˜ì—­ ê±´ë„ˆë›°ê¸°
        if not content_started:
            if line.strip() == '' and len(content_lines) == 0:
                continue
            # í—¤ë” íŒ¨í„´ í™•ì¸
            if re.match(r'^(From:|To:|Cc:|Bcc:|Subject:|Date:|Message-ID:|X-|Mime-Version:|Content-)', line, re.I):
                continue
            content_started = True

        content_lines.append(line)

    text = '\n'.join(content_lines)

    # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)

    return text.strip()


def classify_email_type(text: str, subject: str = "") -> str:
    """ì´ë©”ì¼ ìœ í˜• ìë™ ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜)"""
    combined = f"{subject} {text}".lower()

    # ì±„ìš© ê´€ë ¨
    if any(kw in combined for kw in ['interview', 'resume', 'job', 'position', 'hire', 'candidate', 'recruitment']):
        return "ì±„ìš©"

    # ë§ˆì¼€íŒ…/í”„ë¡œëª¨ì…˜
    if any(kw in combined for kw in ['sale', 'discount', 'offer', 'promotion', 'subscribe', 'newsletter', 'unsubscribe']):
        return "ë§ˆì¼€íŒ…"

    # ê³µì§€/ì•Œë¦¼
    if any(kw in combined for kw in ['announcement', 'notice', 'update', 'reminder', 'alert', 'notification', 'policy']):
        return "ê³µì§€"

    # ê°œì¸ì  ë©”ì‹œì§€
    if any(kw in combined for kw in ['thank', 'please', 'help', 'question', 'meeting', 'lunch', 'dinner', 'call']):
        return "ê°œì¸"

    return "ê¸°íƒ€"


def estimate_importance(text: str, subject: str = "") -> int:
    """ì¤‘ìš”ë„ ì¶”ì • (1-10)"""
    combined = f"{subject} {text}".lower()
    score = 5  # ê¸°ë³¸ ì ìˆ˜

    # ê¸´ê¸‰/ì¤‘ìš” í‚¤ì›Œë“œ
    if any(kw in combined for kw in ['urgent', 'important', 'asap', 'immediately', 'critical', 'deadline']):
        score += 3

    # ìš”ì²­/ì•¡ì…˜ í•„ìš”
    if any(kw in combined for kw in ['please', 'need', 'require', 'must', 'action required']):
        score += 1

    # ìë™ ë°œì†¡ ë©”ì¼ (ë‚®ì€ ì¤‘ìš”ë„)
    if any(kw in combined for kw in ['automated', 'do not reply', 'no-reply', 'unsubscribe']):
        score -= 2

    return max(1, min(10, score))


def needs_reply(text: str) -> bool:
    """ë‹µë³€ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
    text_lower = text.lower()

    # ì§ˆë¬¸ íŒ¨í„´
    if '?' in text:
        return True

    # ìš”ì²­ íŒ¨í„´
    if any(kw in text_lower for kw in ['please let me know', 'can you', 'could you', 'would you', 'get back to me']):
        return True

    # ìë™ ë°œì†¡ì€ ë‹µë³€ ë¶ˆí•„ìš”
    if any(kw in text_lower for kw in ['do not reply', 'no-reply', 'automated']):
        return False

    return False


def process_enron_dataset(max_samples: int = 10000) -> List[Dict]:
    """
    Enron ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬

    Args:
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ë©”ëª¨ë¦¬ ê´€ë¦¬)

    Returns:
        ì²˜ë¦¬ëœ ì´ë©”ì¼ ë¦¬ìŠ¤íŠ¸
    """
    print(f"ğŸ“¥ Enron ì´ë©”ì¼ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ìµœëŒ€ {max_samples}ê°œ)")

    # corbt/enron-emails ë°ì´í„°ì…‹ ìš°ì„  ì‚¬ìš© (ë” ì•ˆì •ì )
    try:
        dataset = load_dataset(
            "corbt/enron-emails",
            split=f"train[:{max_samples}]"
        )
        print(f"âœ… {len(dataset)}ê°œ ì´ë©”ì¼ ë¡œë“œ ì™„ë£Œ (corbt/enron-emails)")
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

    processed_emails = []

    print("ğŸ”„ ì´ë©”ì¼ ì „ì²˜ë¦¬ ì¤‘...")
    for i, item in enumerate(dataset):
        try:
            # corbt/enron-emails í˜•ì‹: body, subject í•„ë“œ ì‚¬ìš©
            text = item.get('body', item.get('text', item.get('content', '')))
            subject = item.get('subject', '') or ''

            # í…ìŠ¤íŠ¸ ì •ì œ
            cleaned_text = clean_email_text(text)

            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì´ë©”ì¼ ì œì™¸
            if len(cleaned_text) < 50 or len(cleaned_text) > 5000:
                continue

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ/ìƒì„±
            email_data = {
                "id": f"enron_{i:06d}",
                "text": cleaned_text,
                "subject": subject if subject else cleaned_text[:50] + "...",
                "email_type": classify_email_type(cleaned_text, subject),
                "importance_score": estimate_importance(cleaned_text, subject),
                "needs_reply": needs_reply(cleaned_text),
                "sentiment": "neutral",  # ê¸°ë³¸ê°’
                "source": "enron"
            }

            processed_emails.append(email_data)

            if (i + 1) % 1000 == 0:
                print(f"  ì²˜ë¦¬ ì¤‘: {i + 1}/{len(dataset)}")

        except Exception as e:
            continue

    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_emails)}ê°œ ì´ë©”ì¼")

    return processed_emails


def save_processed_data(emails: List[Dict], filename: str = "enron_processed.json"):
    """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    output_path = DATA_DIR / filename

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            "created_at": datetime.now().isoformat(),
            "total_count": len(emails),
            "emails": emails
        }, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"   - ì´ {len(emails)}ê°œ ì´ë©”ì¼")

    # ìœ í˜•ë³„ í†µê³„
    type_counts = {}
    for email in emails:
        t = email['email_type']
        type_counts[t] = type_counts.get(t, 0) + 1

    print("   - ìœ í˜•ë³„ ë¶„í¬:")
    for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):
        print(f"     {t}: {count}ê°œ ({count/len(emails)*100:.1f}%)")


def create_email_type_samples(emails: List[Dict], samples_per_type: int = 50):
    """
    ì´ë©”ì¼ ìœ í˜•ë³„ ëŒ€í‘œ ìƒ˜í”Œ ì¶”ì¶œ (RAG ê²€ìƒ‰ìš©)
    """
    type_samples = {}

    for email in emails:
        email_type = email['email_type']
        if email_type not in type_samples:
            type_samples[email_type] = []

        if len(type_samples[email_type]) < samples_per_type:
            type_samples[email_type].append(email)

    # ì €ì¥
    samples_path = DATA_DIR / "email_type_samples.json"
    with open(samples_path, 'w', encoding='utf-8') as f:
        json.dump(type_samples, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ìœ í˜•ë³„ ìƒ˜í”Œ ì €ì¥: {samples_path}")
    for t, samples in type_samples.items():
        print(f"   - {t}: {len(samples)}ê°œ")

    return type_samples


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ“§ Enron Email Dataset ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬")
    print("=" * 60)

    # 1. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
    emails = process_enron_dataset(max_samples=10000)

    if not emails:
        print("âŒ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹¤íŒ¨")
        return

    # 2. ì „ì²´ ë°ì´í„° ì €ì¥
    save_processed_data(emails)

    # 3. ìœ í˜•ë³„ ìƒ˜í”Œ ì¶”ì¶œ
    create_email_type_samples(emails)

    print("\n" + "=" * 60)
    print("âœ… ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„: ChromaDBì— ë²¡í„° ì €ì¥")
    print("   python -m src.rag.build_vectordb")
    print("=" * 60)


if __name__ == "__main__":
    main()
